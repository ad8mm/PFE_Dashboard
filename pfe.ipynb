{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2158c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.trend import MACD\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Chargement des donnÃ©es\n",
    "df = pd.read_csv(\"aapl_data.csv\").dropna()\n",
    "\n",
    "# 2. Enrichissement des indicateurs (rÃ©duit pour Ã©viter le surapprentissage)\n",
    "df[\"Momentum_5\"] = df[\"Close\"] - df[\"Close\"].shift(5)\n",
    "df[\"Volatility_5\"] = df[\"Close\"].rolling(5).std()\n",
    "df[\"Return1d\"] = df[\"Close\"].pct_change()\n",
    "df[\"MA_10\"] = df[\"Close\"].rolling(10).mean()\n",
    "df[\"MA_50\"] = df[\"Close\"].rolling(50).mean()\n",
    "df[\"MA_200\"] = df[\"Close\"].rolling(200).mean()\n",
    "df[\"MA_ratio\"] = df[\"MA_10\"] / df[\"MA_50\"]\n",
    "df[\"Long_trend\"] = (df[\"Close\"] - df[\"MA_200\"]) / df[\"MA_200\"]  # NormalisÃ©\n",
    "df[\"Price_vs_High\"] = df[\"Close\"] / df[\"High\"].rolling(10).max()\n",
    "df[\"Price_vs_Low\"] = df[\"Close\"] / df[\"Low\"].rolling(10).min()\n",
    "df[\"Trend_day\"] = (df[\"Close\"] - df[\"Open\"]) / df[\"Open\"]  # NormalisÃ©\n",
    "df[\"RSI\"] = RSIIndicator(df[\"Close\"]).rsi()\n",
    "macd = MACD(df[\"Close\"])\n",
    "df[\"MACD\"] = macd.macd()\n",
    "df[\"MACD_signal\"] = macd.macd_signal()\n",
    "df[\"MACD_diff\"] = macd.macd_diff()\n",
    "\n",
    "# Normalisation du volume\n",
    "df[\"Volume_norm\"] = df[\"Volume\"] / df[\"Volume\"].rolling(20).mean()\n",
    "\n",
    "# 3. Cible : log-return Ã  J+5\n",
    "df[\"Target\"] = np.log(df[\"Close\"].shift(-5)) - np.log(df[\"Close\"])\n",
    "df = df.dropna()\n",
    "\n",
    "# 4. Features + split (sÃ©lection plus conservatrice)\n",
    "features = [\"Momentum_5\", \"Volatility_5\", \"Return1d\", \"MA_ratio\",\n",
    "            \"Price_vs_High\", \"Price_vs_Low\", \"Trend_day\", \"Long_trend\",\n",
    "            \"RSI\", \"MACD_diff\", \"Volume_norm\"]\n",
    "\n",
    "X = df[features]\n",
    "y = df[\"Target\"]\n",
    "\n",
    "# Suppression des valeurs aberrantes (outliers)\n",
    "Q1 = y.quantile(0.25)\n",
    "Q3 = y.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "mask = (y >= lower_bound) & (y <= upper_bound)\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "print(f\"ğŸ“Š DonnÃ©es aprÃ¨s suppression des outliers: {len(X)} observations\")\n",
    "\n",
    "# Split temporel\n",
    "split_date = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_date], X.iloc[split_date:]\n",
    "y_train, y_test = y.iloc[:split_date], y.iloc[split_date:]\n",
    "\n",
    "# Normalisation des features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# CorrÃ©lation entre features (avec seuil plus strict)\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr_matrix = X_train_scaled.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True, linewidths=0.5)\n",
    "plt.title(\"ğŸ” CorrÃ©lation entre les variables explicatives\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Suppression des features trop corrÃ©lÃ©es (seuil plus strict)\n",
    "corr_threshold = 0.35  # Plus strict\n",
    "corr_matrix_abs = X_train_scaled.corr().abs()\n",
    "upper_tri = corr_matrix_abs.where(np.triu(np.ones(corr_matrix_abs.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > corr_threshold)]\n",
    "print(\"ğŸš« Variables supprimÃ©es pour forte corrÃ©lation :\", to_drop)\n",
    "\n",
    "X_train_final = X_train_scaled.drop(columns=to_drop)\n",
    "X_test_final = X_test_scaled.drop(columns=to_drop)\n",
    "\n",
    "print(f\"ğŸ¯ Nombre final de features: {X_train_final.shape[1]}\")\n",
    "\n",
    "# 5. HyperparamÃ¨tres optimisÃ©s contre le surapprentissage\n",
    "optimized_params = {\n",
    "    \"n_estimators\": 200,        # RÃ©duit de 300\n",
    "    \"learning_rate\": 0.05,      # AugmentÃ© de 0.01\n",
    "    \"max_depth\": 3,             # RÃ©duit de 2 Ã  3 (plus de complexitÃ© contrÃ´lÃ©e)\n",
    "    \"subsample\": 0.8,           # AugmentÃ© de 0.7\n",
    "    \"colsample_bytree\": 0.8,    # AugmentÃ© de 0.7\n",
    "    \"colsample_bylevel\": 0.8,   # Nouveau paramÃ¨tre\n",
    "    \"reg_lambda\": 10.0,         # AugmentÃ© de 2.0 (plus de rÃ©gularisation L2)\n",
    "    \"reg_alpha\": 5.0,           # AugmentÃ© de 1.0 (plus de rÃ©gularisation L1)\n",
    "    \"min_child_weight\": 3,      # Nouveau paramÃ¨tre pour Ã©viter le surapprentissage\n",
    "    \"gamma\": 0.1,               # Nouveau paramÃ¨tre (minimum split loss)\n",
    "    \"random_state\": 42,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\"\n",
    "}\n",
    "\n",
    "# 6. Validation croisÃ©e temporelle avec plus de splits\n",
    "tscv = TimeSeriesSplit(n_splits=8)  # AugmentÃ© de 5 Ã  8\n",
    "cv_scores = []\n",
    "val_scores = []\n",
    "\n",
    "print(\"\\nğŸ” Validation croisÃ©e temporelle:\")\n",
    "for i, (train_idx, val_idx) in enumerate(tscv.split(X_train_final)):\n",
    "    X_tr, X_val = X_train_final.iloc[train_idx], X_train_final.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    model_cv = XGBRegressor(**optimized_params, early_stopping_rounds=50)\n",
    "    model_cv.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_tr, y_tr), (X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # PrÃ©dictions\n",
    "    train_preds = model_cv.predict(X_tr)\n",
    "    val_preds = model_cv.predict(X_val)\n",
    "\n",
    "    # Scores\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_tr, train_preds))\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "\n",
    "    cv_scores.append(train_rmse)\n",
    "    val_scores.append(val_rmse)\n",
    "\n",
    "    print(f\"Split {i+1} - Train RMSE: {train_rmse:.5f}, Val RMSE: {val_rmse:.5f}, \"\n",
    "          f\"Ã‰cart: {val_rmse - train_rmse:.5f}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š RMSE moyen Train: {np.mean(cv_scores):.5f}\")\n",
    "print(f\"ğŸ“Š RMSE moyen Validation: {np.mean(val_scores):.5f}\")\n",
    "print(f\"ğŸ“Š Ã‰cart moyen (surapprentissage): {np.mean(val_scores) - np.mean(cv_scores):.5f}\")\n",
    "\n",
    "# 7. EntraÃ®nement final avec early stopping plus agressif\n",
    "final_model = XGBRegressor(**optimized_params)\n",
    "final_model.fit(\n",
    "    X_train_final, y_train,\n",
    "    eval_set=[(X_train_final, y_train), (X_test_final, y_test)],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ¯ Nombre d'arbres utilisÃ©s: {final_model}\")\n",
    "\n",
    "# 8. PrÃ©diction & reconstruction prix J+5\n",
    "y_pred_delta = final_model.predict(X_test_final)\n",
    "close_today = df[\"Close\"].iloc[-len(y_test):].values\n",
    "y_pred_price = close_today * np.exp(y_pred_delta)\n",
    "y_true_price = close_today * np.exp(y_test.values)\n",
    "\n",
    "# 9. Ã‰valuation dÃ©taillÃ©e\n",
    "# Sur les prix\n",
    "mae_price = mean_absolute_error(y_true_price, y_pred_price)\n",
    "rmse_price = np.sqrt(mean_squared_error(y_true_price, y_pred_price))\n",
    "r2_price = r2_score(y_true_price, y_pred_price)\n",
    "\n",
    "# Sur les log-returns\n",
    "mae_log = mean_absolute_error(y_test, y_pred_delta)\n",
    "rmse_log = np.sqrt(mean_squared_error(y_test, y_pred_delta))\n",
    "r2_log = r2_score(y_test, y_pred_delta)\n",
    "\n",
    "# Ã‰valuation sur train pour dÃ©tecter le surapprentissage\n",
    "y_pred_train_delta = final_model.predict(X_train_final)\n",
    "mae_train_log = mean_absolute_error(y_train, y_pred_train_delta)\n",
    "rmse_train_log = np.sqrt(mean_squared_error(y_train, y_pred_train_delta))\n",
    "r2_train_log = r2_score(y_train, y_pred_train_delta)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ“ˆ RÃ‰SULTATS FINAUX\")\n",
    "print(\"=\"*50)\n",
    "print(f\"ğŸ‹ï¸  TRAIN - RMSE: {rmse_train_log:.5f}, MAE: {mae_train_log:.5f}, RÂ²: {r2_train_log:.5f}\")\n",
    "print(f\"ğŸ§ª TEST  - RMSE: {rmse_log:.5f}, MAE: {mae_log:.5f}, RÂ²: {r2_log:.5f}\")\n",
    "print(f\"ğŸ“Š Ã‰CART (surapprentissage): {rmse_log - rmse_train_log:.5f}\")\n",
    "\n",
    "print(f\"\\nğŸ’° Sur les prix:\")\n",
    "print(f\"ğŸ“Š MAE  : ${mae_price:.2f}\")\n",
    "print(f\"ğŸ“‰ RMSE : ${rmse_price:.2f}\")\n",
    "print(f\"ğŸ“ˆ RÂ²   : {r2_price:.4f}\")\n",
    "\n",
    "# 10. Visualisation amÃ©liorÃ©e\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Prix prÃ©dits vs rÃ©els\n",
    "axes[0,0].plot(y_true_price, label=\"Prix RÃ©el J+5\", linewidth=2, alpha=0.8)\n",
    "axes[0,0].plot(y_pred_price, label=\"PrÃ©vision J+5\", linestyle='--', alpha=0.8)\n",
    "axes[0,0].set_title(\"ğŸ“ˆ PrÃ©diction du prix AAPL Ã  J+5\")\n",
    "axes[0,0].set_xlabel(\"Observation\")\n",
    "axes[0,0].set_ylabel(\"Prix ($)\")\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot prix\n",
    "axes[0,1].scatter(y_true_price, y_pred_price, alpha=0.6, s=20)\n",
    "axes[0,1].plot([y_true_price.min(), y_true_price.max()],\n",
    "               [y_true_price.min(), y_true_price.max()], 'r--', lw=2)\n",
    "axes[0,1].set_xlabel(\"Prix RÃ©el ($)\")\n",
    "axes[0,1].set_ylabel(\"Prix PrÃ©dit ($)\")\n",
    "axes[0,1].set_title(f\"ğŸ“Š CorrÃ©lation Prix (RÂ² = {r2_price:.3f})\")\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Log-returns\n",
    "axes[1,0].plot(y_test.values, label=\"Log-return RÃ©el\", linewidth=2, alpha=0.8)\n",
    "axes[1,0].plot(y_pred_delta, label=\"Log-return PrÃ©dit\", linestyle='--', alpha=0.8)\n",
    "axes[1,0].set_title(\"ğŸ“ˆ PrÃ©diction des Log-returns\")\n",
    "axes[1,0].set_xlabel(\"Observation\")\n",
    "axes[1,0].set_ylabel(\"Log-return\")\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# RÃ©sidus\n",
    "residuals = y_test.values - y_pred_delta\n",
    "axes[1,1].scatter(y_pred_delta, residuals, alpha=0.6, s=20)\n",
    "axes[1,1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1,1].set_xlabel(\"PrÃ©dictions\")\n",
    "axes[1,1].set_ylabel(\"RÃ©sidus\")\n",
    "axes[1,1].set_title(\"ğŸ“Š Analyse des rÃ©sidus\")\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 12. Comparaison avec baseline\n",
    "close_today_test = df[\"Close\"].iloc[-len(y_test):].values\n",
    "y_naive_price = close_today_test\n",
    "r2_naive = r2_score(y_true_price, y_naive_price)\n",
    "mae_naive = mean_absolute_error(y_true_price, y_naive_price)\n",
    "rmse_naive = np.sqrt(mean_squared_error(y_true_price, y_naive_price))\n",
    "\n",
    "print(\"\\nğŸ§Š BASELINE NAÃVE (prix constant)\")\n",
    "print(f\"ğŸ“‰ RMSE : ${rmse_naive:.2f}\")\n",
    "print(f\"ğŸ“Š MAE  : ${mae_naive:.2f}\")\n",
    "print(f\"ğŸ“ˆ RÂ²   : {r2_naive:.4f}\")\n",
    "\n",
    "# 13. MÃ©triques de gÃ©nÃ©ralisation\n",
    "print(\"\\nğŸ¯ MÃ‰TRIQUES DE GÃ‰NÃ‰RALISATION\")\n",
    "print(f\"ğŸ“Š AmÃ©lioration vs Baseline (RMSE): {((rmse_naive - rmse_price) / rmse_naive * 100):.1f}%\")\n",
    "print(f\"ğŸ“Š AmÃ©lioration vs Baseline (MAE): {((mae_naive - mae_price) / mae_naive * 100):.1f}%\")\n",
    "\n",
    "if rmse_log - rmse_train_log < 0.001:\n",
    "    print(\"âœ… Pas de surapprentissage dÃ©tectÃ©\")\n",
    "elif rmse_log - rmse_train_log < 0.005:\n",
    "    print(\"âš ï¸  LÃ©ger surapprentissage\")\n",
    "else:\n",
    "    print(\"âŒ Surapprentissage dÃ©tectÃ© - ajuster les hyperparamÃ¨tres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cfa73e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ta\n",
      "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/pc/venv/lib/python3.12/site-packages (from ta) (2.1.3)\n",
      "Requirement already satisfied: pandas in /home/pc/venv/lib/python3.12/site-packages (from ta) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/pc/venv/lib/python3.12/site-packages (from pandas->ta) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/pc/venv/lib/python3.12/site-packages (from pandas->ta) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/pc/venv/lib/python3.12/site-packages (from pandas->ta) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/pc/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n",
      "Building wheels for collected packages: ta\n",
      "  Building wheel for ta (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29482 sha256=5efb52531fcd13f4ddf1eba7f8f90beabb4ba5b774a8f64459ef88796d51a7ea\n",
      "  Stored in directory: /home/pc/.cache/pip/wheels/5c/a1/5f/c6b85a7d9452057be4ce68a8e45d77ba34234a6d46581777c6\n",
      "Successfully built ta\n",
      "Installing collected packages: ta\n",
      "Successfully installed ta-0.11.0\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mta\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmomentum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RSIIndicator\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mta\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MACD\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBRegressor, plot_importance\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TimeSeriesSplit, train_test_split\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_squared_error, r2_score\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "!pip install ta\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.trend import MACD\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler  # Plus robuste que StandardScaler\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import median_abs_deviation\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def detect_outliers_mad(data, threshold=3):\n",
    "    \"\"\"DÃ©tection d'outliers avec MAD (Median Absolute Deviation) - plus robuste que z-score\"\"\"\n",
    "    mad = median_abs_deviation(data, nan_policy='omit')\n",
    "    median = np.median(data)\n",
    "    modified_z_scores = 0.6745 * (data - median) / mad\n",
    "    return np.abs(modified_z_scores) > threshold\n",
    "\n",
    "def winsorize_series(series, lower_percentile=5, upper_percentile=95):\n",
    "    \"\"\"Winsorisation pour limiter l'impact des outliers extrÃªmes\"\"\"\n",
    "    lower_bound = np.percentile(series, lower_percentile)\n",
    "    upper_bound = np.percentile(series, upper_percentile)\n",
    "    return np.clip(series, lower_bound, upper_bound)\n",
    "\n",
    "def robust_feature_engineering(df):\n",
    "    \"\"\"Feature engineering robuste avec gestion des outliers\"\"\"\n",
    "\n",
    "    # Features de base avec winsorisation\n",
    "    df[\"Momentum_5\"] = df[\"Close\"] - df[\"Close\"].shift(5)\n",
    "    df[\"Momentum_5\"] = winsorize_series(df[\"Momentum_5\"])\n",
    "\n",
    "    df[\"Volatility_5\"] = df[\"Close\"].rolling(5).std()\n",
    "    df[\"Volatility_5\"] = winsorize_series(df[\"Volatility_5\"])\n",
    "\n",
    "    df[\"Return1d\"] = df[\"Close\"].pct_change()\n",
    "    df[\"Return1d\"] = winsorize_series(df[\"Return1d\"])\n",
    "\n",
    "    # Moyennes mobiles avec ratio robuste\n",
    "    df[\"MA_10\"] = df[\"Close\"].rolling(10).mean()\n",
    "    df[\"MA_50\"] = df[\"Close\"].rolling(50).mean()\n",
    "    df[\"MA_200\"] = df[\"Close\"].rolling(200).mean()\n",
    "\n",
    "    # Ratios limitÃ©s pour Ã©viter les valeurs extrÃªmes\n",
    "    df[\"MA_ratio\"] = np.clip(df[\"MA_10\"] / df[\"MA_50\"], 0.5, 2.0)\n",
    "\n",
    "    # Tendance long terme normalisÃ©e et bornÃ©e\n",
    "    df[\"Long_trend\"] = np.clip((df[\"Close\"] - df[\"MA_200\"]) / df[\"MA_200\"], -0.5, 0.5)\n",
    "\n",
    "    # Prix relatifs bornÃ©s\n",
    "    df[\"Price_vs_High\"] = np.clip(df[\"Close\"] / df[\"High\"].rolling(10).max(), 0.7, 1.0)\n",
    "    df[\"Price_vs_Low\"] = np.clip(df[\"Close\"] / df[\"Low\"].rolling(10).min(), 1.0, 1.5)\n",
    "\n",
    "    # Trend day normalisÃ© et bornÃ©\n",
    "    trend_day = (df[\"Close\"] - df[\"Open\"]) / df[\"Open\"]\n",
    "    df[\"Trend_day\"] = np.clip(trend_day, -0.1, 0.1)\n",
    "\n",
    "    # RSI (dÃ©jÃ  bornÃ© entre 0-100)\n",
    "    df[\"RSI\"] = RSIIndicator(df[\"Close\"]).rsi()\n",
    "    df[\"RSI\"] = df[\"RSI\"].fillna(50)  # Valeur neutre pour les NaN\n",
    "\n",
    "    # MACD avec winsorisation\n",
    "    macd = MACD(df[\"Close\"])\n",
    "    df[\"MACD_diff\"] = macd.macd_diff()\n",
    "    df[\"MACD_diff\"] = winsorize_series(df[\"MACD_diff\"].fillna(0))\n",
    "\n",
    "    # Volume normalisÃ© robuste\n",
    "    volume_ma = df[\"Volume\"].rolling(20).median()  # MÃ©diane plus robuste que moyenne\n",
    "    df[\"Volume_norm\"] = np.clip(df[\"Volume\"] / volume_ma, 0.1, 5.0)\n",
    "\n",
    "    return df\n",
    "\n",
    "# 1. Chargement et preprocessing robuste\n",
    "df = pd.read_csv(\"aapl_data.csv\").dropna()\n",
    "\n",
    "# Application du feature engineering robuste\n",
    "df = robust_feature_engineering(df)\n",
    "\n",
    "# 2. Cible avec limites pour Ã©viter les valeurs extrÃªmes\n",
    "df[\"Target\"] = np.log(df[\"Close\"].shift(-5)) - np.log(df[\"Close\"])\n",
    "# Limitation des log-returns extrÃªmes\n",
    "df[\"Target\"] = np.clip(df[\"Target\"], -0.3, 0.3)\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"ğŸ“Š DonnÃ©es aprÃ¨s preprocessing: {len(df)} observations\")\n",
    "\n",
    "# 3. SÃ©lection des features robustes\n",
    "robust_features = [\"Momentum_5\", \"Volatility_5\", \"Return1d\", \"MA_ratio\",\n",
    "                   \"Price_vs_High\", \"Price_vs_Low\", \"Trend_day\", \"Long_trend\",\n",
    "                   \"RSI\", \"MACD_diff\", \"Volume_norm\"]\n",
    "\n",
    "X = df[robust_features]\n",
    "y = df[\"Target\"]\n",
    "\n",
    "# 4. DÃ©tection et traitement des outliers avec MAD\n",
    "print(\"\\nğŸ” DÃ©tection des outliers avec MAD:\")\n",
    "outlier_mask = np.zeros(len(y), dtype=bool)\n",
    "\n",
    "for col in X.columns:\n",
    "    col_outliers = detect_outliers_mad(X[col].values)\n",
    "    outlier_mask |= col_outliers\n",
    "    print(f\"  {col}: {col_outliers.sum()} outliers dÃ©tectÃ©s\")\n",
    "\n",
    "target_outliers = detect_outliers_mad(y.values)\n",
    "outlier_mask |= target_outliers\n",
    "print(f\"  Target: {target_outliers.sum()} outliers dÃ©tectÃ©s\")\n",
    "\n",
    "print(f\"ğŸ“Š Total outliers: {outlier_mask.sum()}/{len(y)} ({outlier_mask.sum()/len(y)*100:.1f}%)\")\n",
    "\n",
    "# Conservation des donnÃ©es non-outliers\n",
    "X_clean = X[~outlier_mask]\n",
    "y_clean = y[~outlier_mask]\n",
    "print(f\"ğŸ“Š DonnÃ©es finales: {len(X_clean)} observations\")\n",
    "\n",
    "# 5. Split temporel\n",
    "split_date = int(len(X_clean) * 0.8)\n",
    "X_train, X_test = X_clean.iloc[:split_date], X_clean.iloc[split_date:]\n",
    "y_train, y_test = y_clean.iloc[:split_date], y_clean.iloc[split_date:]\n",
    "\n",
    "# 6. Normalisation robuste (RobustScaler rÃ©siste mieux aux outliers)\n",
    "robust_scaler = RobustScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    robust_scaler.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    robust_scaler.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# 7. Analyse de corrÃ©lation pour la sÃ©lection finale (approche plus intelligente)\n",
    "corr_matrix = X_train_scaled.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# CorrÃ©lation entre features (avec seuil plus strict)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True, linewidths=0.5)\n",
    "plt.title(\"ğŸ” CorrÃ©lation entre les variables explicatives\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# SÃ©lection intelligente : on garde les features les plus importantes\n",
    "print(f\"\\nğŸ” Matrice de corrÃ©lation:\")\n",
    "print(corr_matrix.round(2))\n",
    "\n",
    "# StratÃ©gie : supprimer seulement si corrÃ©lation > 0.7 ET garder minimum 8 features\n",
    "corr_threshold = 0.7\n",
    "highly_correlated_pairs = []\n",
    "\n",
    "for col in upper_tri.columns:\n",
    "    correlated_features = upper_tri.index[upper_tri[col] > corr_threshold].tolist()\n",
    "    if correlated_features:\n",
    "        highly_correlated_pairs.extend([(col, feat, upper_tri.loc[feat, col]) for feat in correlated_features])\n",
    "\n",
    "# Tri par corrÃ©lation dÃ©croissante\n",
    "highly_correlated_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "print(f\"\\nğŸ” Paires trÃ¨s corrÃ©lÃ©es (>{corr_threshold}):\")\n",
    "for pair in highly_correlated_pairs[:5]:  # Top 5\n",
    "    print(f\"  {pair[0]} - {pair[1]}: {pair[2]:.3f}\")\n",
    "\n",
    "# Suppression conservative : maximum 3 features\n",
    "to_drop = []\n",
    "if highly_correlated_pairs:\n",
    "    # On ne supprime que si on a plus de 8 features et corrÃ©lation > 0.8\n",
    "    for pair in highly_correlated_pairs:\n",
    "        if len(X_train_scaled.columns) - len(to_drop) > 8 and pair[2] > 0.8:\n",
    "            if pair[1] not in to_drop:  # Ã‰viter les doublons\n",
    "                to_drop.append(pair[1])\n",
    "        if len(to_drop) >= 2:  # Maximum 2 suppressions\n",
    "            break\n",
    "\n",
    "print(f\"\\nğŸš« Variables supprimÃ©es pour forte corrÃ©lation (>{0.8}): {to_drop}\")\n",
    "\n",
    "X_train_final = X_train_scaled.drop(columns=to_drop) if to_drop else X_train_scaled\n",
    "X_test_final = X_test_scaled.drop(columns=to_drop) if to_drop else X_test_scaled\n",
    "print(f\"ğŸ¯ Features finales ({len(X_train_final.columns)}): {list(X_train_final.columns)}\")\n",
    "\n",
    "# 8. HyperparamÃ¨tres Ã©quilibrÃ©s pour permettre l'apprentissage\n",
    "ultra_robust_params = {\n",
    "    \"n_estimators\": 300,        # Suffisant pour l'apprentissage\n",
    "    \"learning_rate\": 0.05,      # Apprentissage modÃ©rÃ©\n",
    "    \"max_depth\": 4,             # ComplexitÃ© contrÃ´lÃ©e mais suffisante\n",
    "    \"subsample\": 0.8,           # Ã‰chantillonnage robuste\n",
    "    \"colsample_bytree\": 0.8,    # SÃ©lection de features robuste\n",
    "    \"colsample_bylevel\": 0.8,\n",
    "    \"reg_lambda\": 5.0,          # RÃ©gularisation modÃ©rÃ©e (Ã©tait trop forte)\n",
    "    \"reg_alpha\": 2.0,           # RÃ©gularisation modÃ©rÃ©e (Ã©tait trop forte)\n",
    "    \"min_child_weight\": 3,      # RÃ©duit pour permettre plus de splits\n",
    "    \"gamma\": 0.05,              # RÃ©duit pour permettre plus de splits\n",
    "    \"max_delta_step\": 0,        # Pas de limite (0 = pas de limite)\n",
    "    \"random_state\": 42,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": [\"rmse\", \"mae\"]  # Les deux mÃ©triques pour surveillance\n",
    "}\n",
    "\n",
    "# 9. Validation croisÃ©e robuste avec bootstrapping\n",
    "def robust_cross_validation(X, y, params, n_splits=5):  # RÃ©duit Ã  5 splits\n",
    "    \"\"\"Validation croisÃ©e avec mesures de robustesse\"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits, test_size=len(X)//10)  # Test size dÃ©fini\n",
    "\n",
    "    mae_scores = []\n",
    "    rmse_scores = []\n",
    "    r2_scores = []\n",
    "\n",
    "    print(f\"ğŸ” Validation croisÃ©e avec {n_splits} splits:\")\n",
    "\n",
    "    for i, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        print(f\"  Split {i+1}: Train={len(X_tr)}, Val={len(X_val)}\")\n",
    "\n",
    "        model = XGBRegressor(**params, early_stopping_rounds=30)\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "        preds = model.predict(X_val)\n",
    "\n",
    "        mae = mean_absolute_error(y_val, preds)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        r2 = r2_score(y_val, preds)\n",
    "\n",
    "        mae_scores.append(mae)\n",
    "        rmse_scores.append(rmse)\n",
    "        r2_scores.append(r2)\n",
    "\n",
    "        print(f\"    MAE: {mae:.5f}, RMSE: {rmse:.5f}, RÂ²: {r2:.5f}\")\n",
    "\n",
    "    return {\n",
    "        'mae_mean': np.mean(mae_scores),\n",
    "        'mae_std': np.std(mae_scores),\n",
    "        'rmse_mean': np.mean(rmse_scores),\n",
    "        'rmse_std': np.std(rmse_scores),\n",
    "        'r2_mean': np.mean(r2_scores),\n",
    "        'r2_std': np.std(r2_scores)\n",
    "    }\n",
    "\n",
    "print(\"\\nğŸ” Validation croisÃ©e robuste en cours...\")\n",
    "cv_results = robust_cross_validation(X_train_final, y_train, ultra_robust_params)\n",
    "\n",
    "print(f\"\\nğŸ“Š RÃ‰SULTATS VALIDATION CROISÃ‰E:\")\n",
    "print(f\"MAE  : {cv_results['mae_mean']:.5f} Â± {cv_results['mae_std']:.5f}\")\n",
    "print(f\"RMSE : {cv_results['rmse_mean']:.5f} Â± {cv_results['rmse_std']:.5f}\")\n",
    "print(f\"RÂ²   : {cv_results['r2_mean']:.5f} Â± {cv_results['r2_std']:.5f}\")\n",
    "\n",
    "# 10. EntraÃ®nement du modÃ¨le final avec surveillance\n",
    "print(f\"\\nğŸ¯ EntraÃ®nement du modÃ¨le final avec {len(X_train_final.columns)} features\")\n",
    "print(f\"ğŸ“Š Taille train: {len(X_train_final)}, Taille test: {len(X_test_final)}\")\n",
    "\n",
    "# VÃ©rification des donnÃ©es avant entraÃ®nement\n",
    "print(f\"ğŸ“Š Plage cible train: [{y_train.min():.4f}, {y_train.max():.4f}]\")\n",
    "print(f\"ğŸ“Š Plage cible test: [{y_test.min():.4f}, {y_test.max():.4f}]\")\n",
    "\n",
    "final_model = XGBRegressor(**ultra_robust_params, early_stopping_rounds=50)  # Plus tolÃ©rant\n",
    "final_model.fit(\n",
    "    X_train_final, y_train,\n",
    "    eval_set=[(X_train_final, y_train), (X_test_final, y_test)],\n",
    "    verbose=10  # Affichage toutes les 10 itÃ©rations pour diagnostic\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ¯ ModÃ¨le entraÃ®nÃ© avec {final_model.best_iteration} arbres\")\n",
    "print(f\"ğŸ¯ Meilleur score: {final_model.best_score:.5f}\")\n",
    "\n",
    "# 11. Ã‰valuation finale avec mÃ©triques robustes\n",
    "y_pred_train = final_model.predict(X_train_final)\n",
    "y_pred_test = final_model.predict(X_test_final)\n",
    "\n",
    "# MÃ©triques sur log-returns\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "# 12. Test de robustesse sur prix\n",
    "close_train = df[\"Close\"].iloc[:len(y_train)].values\n",
    "close_test = df[\"Close\"].iloc[-len(y_test):].values\n",
    "\n",
    "price_pred_train = close_train * np.exp(y_pred_train)\n",
    "price_true_train = close_train * np.exp(y_train.values)\n",
    "price_pred_test = close_test * np.exp(y_pred_test)\n",
    "price_true_test = close_test * np.exp(y_test.values)\n",
    "\n",
    "price_mae_train = mean_absolute_error(price_true_train, price_pred_train)\n",
    "price_mae_test = mean_absolute_error(price_true_test, price_pred_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ›¡ï¸  Ã‰VALUATION DE ROBUSTESSE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ğŸ“Š TRAIN - MAE: {train_mae:.5f}, RMSE: {train_rmse:.5f}, RÂ²: {train_r2:.5f}\")\n",
    "print(f\"ğŸ§ª TEST  - MAE: {test_mae:.5f}, RMSE: {test_rmse:.5f}, RÂ²: {test_r2:.5f}\")\n",
    "print(f\"ğŸ“ˆ Ã‰CART MAE (robustesse): {abs(test_mae - train_mae):.5f}\")\n",
    "print(f\"ğŸ“ˆ Ã‰CART RMSE (robustesse): {abs(test_rmse - train_rmse):.5f}\")\n",
    "\n",
    "print(f\"\\nğŸ’° PRIX - Train MAE: ${price_mae_train:.2f}, Test MAE: ${price_mae_test:.2f}\")\n",
    "print(f\"ğŸ’° Ã‰CART PRIX: ${abs(price_mae_test - price_mae_train):.2f}\")\n",
    "\n",
    "# 13. Indicateurs de robustesse\n",
    "robustness_score = 1 - abs(test_mae - train_mae) / train_mae\n",
    "stability_score = 1 - cv_results['mae_std'] / cv_results['mae_mean']\n",
    "\n",
    "print(f\"\\nğŸ¯ SCORES DE ROBUSTESSE:\")\n",
    "print(f\"Robustesse Train/Test: {robustness_score:.3f} (1.0 = parfait)\")\n",
    "print(f\"StabilitÃ© CV: {stability_score:.3f} (1.0 = parfait)\")\n",
    "\n",
    "if robustness_score > 0.95 and stability_score > 0.9:\n",
    "    print(\"âœ… MODÃˆLE TRÃˆS ROBUSTE\")\n",
    "elif robustness_score > 0.9 and stability_score > 0.8:\n",
    "    print(\"âœ… MODÃˆLE ROBUSTE\")\n",
    "elif robustness_score > 0.8 and stability_score > 0.7:\n",
    "    print(\"âš ï¸  MODÃˆLE MOYENNEMENT ROBUSTE\")\n",
    "else:\n",
    "    print(\"âŒ MODÃˆLE PEU ROBUSTE - RÃ©vision nÃ©cessaire\")\n",
    "\n",
    "# 14. Visualisations de robustesse\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Performance train vs test\n",
    "axes[0,0].scatter(y_train, y_pred_train, alpha=0.5, s=20, label=f'Train (MAE={train_mae:.4f})')\n",
    "axes[0,0].scatter(y_test, y_pred_test, alpha=0.5, s=20, label=f'Test (MAE={test_mae:.4f})')\n",
    "axes[0,0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "axes[0,0].set_xlabel(\"Valeurs RÃ©elles\")\n",
    "axes[0,0].set_ylabel(\"PrÃ©dictions\")\n",
    "axes[0,0].set_title(\"ğŸ¯ Robustesse Train vs Test\")\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# RÃ©sidus pour dÃ©tecter les patterns\n",
    "residuals_test = y_test.values - y_pred_test\n",
    "axes[0,1].scatter(y_pred_test, residuals_test, alpha=0.6, s=20)\n",
    "axes[0,1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0,1].set_xlabel(\"PrÃ©dictions\")\n",
    "axes[0,1].set_ylabel(\"RÃ©sidus\")\n",
    "axes[0,1].set_title(\"ğŸ“Š Analyse des RÃ©sidus\")\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution des erreurs\n",
    "axes[0,2].hist(residuals_test, bins=30, alpha=0.7, density=True)\n",
    "axes[0,2].axvline(x=0, color='r', linestyle='--')\n",
    "axes[0,2].set_xlabel(\"RÃ©sidus\")\n",
    "axes[0,2].set_ylabel(\"DensitÃ©\")\n",
    "axes[0,2].set_title(\"ğŸ“ˆ Distribution des Erreurs\")\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# Prix prÃ©dits vs rÃ©els\n",
    "axes[1,0].plot(price_true_test, label=\"Prix RÃ©el\", linewidth=2, alpha=0.8)\n",
    "axes[1,0].plot(price_pred_test, label=\"Prix PrÃ©dit\", linestyle='--', alpha=0.8)\n",
    "axes[1,0].set_title(\"ğŸ’° PrÃ©diction des Prix\")\n",
    "axes[1,0].set_xlabel(\"Observation\")\n",
    "axes[1,0].set_ylabel(\"Prix ($)\")\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Importance des features\n",
    "feature_importance = final_model.feature_importances_\n",
    "feature_names = X_train_final.columns\n",
    "sorted_idx = np.argsort(feature_importance)[-10:]\n",
    "axes[1,1].barh(range(len(sorted_idx)), feature_importance[sorted_idx])\n",
    "axes[1,1].set_yticks(range(len(sorted_idx)))\n",
    "axes[1,1].set_yticklabels([feature_names[i] for i in sorted_idx])\n",
    "axes[1,1].set_title(\"ğŸ¯ Importance des Features\")\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Performance temporelle\n",
    "error_abs = np.abs(residuals_test)\n",
    "axes[1,2].plot(error_abs, alpha=0.7)\n",
    "axes[1,2].axhline(y=np.mean(error_abs), color='r', linestyle='--',\n",
    "                  label=f'MAE moyen: {np.mean(error_abs):.4f}')\n",
    "axes[1,2].set_xlabel(\"Observation\")\n",
    "axes[1,2].set_ylabel(\"Erreur Absolue\")\n",
    "axes[1,2].set_title(\"ğŸ“‰ StabilitÃ© Temporelle\")\n",
    "axes[1,2].legend()\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 15. Test de stress (simulation avec bruit)\n",
    "print(\"\\nğŸ§ª TEST DE STRESS - Ajout de bruit gaussien\")\n",
    "noise_levels = [0.01, 0.02, 0.05, 0.1]\n",
    "stress_results = []\n",
    "\n",
    "for noise in noise_levels:\n",
    "    X_test_noisy = X_test_final + np.random.normal(0, noise, X_test_final.shape)\n",
    "    y_pred_noisy = final_model.predict(X_test_noisy)\n",
    "    mae_noisy = mean_absolute_error(y_test, y_pred_noisy)\n",
    "    stress_results.append(mae_noisy)\n",
    "    print(f\"Bruit {noise:.2f}: MAE = {mae_noisy:.5f} (dÃ©gradation: {((mae_noisy-test_mae)/test_mae*100):+.1f}%)\")\n",
    "\n",
    "print(f\"\\nğŸ›¡ï¸  RÃ‰SISTANCE AU BRUIT: DÃ©gradation moyenne {np.mean([(s-test_mae)/test_mae*100 for s in stress_results]):.1f}%\")\n",
    "import os\n",
    "\n",
    "# CrÃ©er le dossier 'model' s'il n'existe pas\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Enregistrement du modÃ¨le\n",
    "joblib.dump(final_model, \"model/final_model.xgb\")\n",
    "\n",
    "# Enregistrement du scaler (pour les nouvelles donnÃ©es)\n",
    "joblib.dump(robust_scaler, \"model/scaler.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
