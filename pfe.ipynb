{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2158c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.trend import MACD\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Chargement des données\n",
    "df = pd.read_csv(\"aapl_data.csv\").dropna()\n",
    "\n",
    "# 2. Enrichissement des indicateurs (réduit pour éviter le surapprentissage)\n",
    "df[\"Momentum_5\"] = df[\"Close\"] - df[\"Close\"].shift(5)\n",
    "df[\"Volatility_5\"] = df[\"Close\"].rolling(5).std()\n",
    "df[\"Return1d\"] = df[\"Close\"].pct_change()\n",
    "df[\"MA_10\"] = df[\"Close\"].rolling(10).mean()\n",
    "df[\"MA_50\"] = df[\"Close\"].rolling(50).mean()\n",
    "df[\"MA_200\"] = df[\"Close\"].rolling(200).mean()\n",
    "df[\"MA_ratio\"] = df[\"MA_10\"] / df[\"MA_50\"]\n",
    "df[\"Long_trend\"] = (df[\"Close\"] - df[\"MA_200\"]) / df[\"MA_200\"]  # Normalisé\n",
    "df[\"Price_vs_High\"] = df[\"Close\"] / df[\"High\"].rolling(10).max()\n",
    "df[\"Price_vs_Low\"] = df[\"Close\"] / df[\"Low\"].rolling(10).min()\n",
    "df[\"Trend_day\"] = (df[\"Close\"] - df[\"Open\"]) / df[\"Open\"]  # Normalisé\n",
    "df[\"RSI\"] = RSIIndicator(df[\"Close\"]).rsi()\n",
    "macd = MACD(df[\"Close\"])\n",
    "df[\"MACD\"] = macd.macd()\n",
    "df[\"MACD_signal\"] = macd.macd_signal()\n",
    "df[\"MACD_diff\"] = macd.macd_diff()\n",
    "\n",
    "# Normalisation du volume\n",
    "df[\"Volume_norm\"] = df[\"Volume\"] / df[\"Volume\"].rolling(20).mean()\n",
    "\n",
    "# 3. Cible : log-return à J+5\n",
    "df[\"Target\"] = np.log(df[\"Close\"].shift(-5)) - np.log(df[\"Close\"])\n",
    "df = df.dropna()\n",
    "\n",
    "# 4. Features + split (sélection plus conservatrice)\n",
    "features = [\"Momentum_5\", \"Volatility_5\", \"Return1d\", \"MA_ratio\",\n",
    "            \"Price_vs_High\", \"Price_vs_Low\", \"Trend_day\", \"Long_trend\",\n",
    "            \"RSI\", \"MACD_diff\", \"Volume_norm\"]\n",
    "\n",
    "X = df[features]\n",
    "y = df[\"Target\"]\n",
    "\n",
    "# Suppression des valeurs aberrantes (outliers)\n",
    "Q1 = y.quantile(0.25)\n",
    "Q3 = y.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "mask = (y >= lower_bound) & (y <= upper_bound)\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "print(f\"📊 Données après suppression des outliers: {len(X)} observations\")\n",
    "\n",
    "# Split temporel\n",
    "split_date = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_date], X.iloc[split_date:]\n",
    "y_train, y_test = y.iloc[:split_date], y.iloc[split_date:]\n",
    "\n",
    "# Normalisation des features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# Corrélation entre features (avec seuil plus strict)\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr_matrix = X_train_scaled.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True, linewidths=0.5)\n",
    "plt.title(\"🔍 Corrélation entre les variables explicatives\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Suppression des features trop corrélées (seuil plus strict)\n",
    "corr_threshold = 0.35  # Plus strict\n",
    "corr_matrix_abs = X_train_scaled.corr().abs()\n",
    "upper_tri = corr_matrix_abs.where(np.triu(np.ones(corr_matrix_abs.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > corr_threshold)]\n",
    "print(\"🚫 Variables supprimées pour forte corrélation :\", to_drop)\n",
    "\n",
    "X_train_final = X_train_scaled.drop(columns=to_drop)\n",
    "X_test_final = X_test_scaled.drop(columns=to_drop)\n",
    "\n",
    "print(f\"🎯 Nombre final de features: {X_train_final.shape[1]}\")\n",
    "\n",
    "# 5. Hyperparamètres optimisés contre le surapprentissage\n",
    "optimized_params = {\n",
    "    \"n_estimators\": 200,        # Réduit de 300\n",
    "    \"learning_rate\": 0.05,      # Augmenté de 0.01\n",
    "    \"max_depth\": 3,             # Réduit de 2 à 3 (plus de complexité contrôlée)\n",
    "    \"subsample\": 0.8,           # Augmenté de 0.7\n",
    "    \"colsample_bytree\": 0.8,    # Augmenté de 0.7\n",
    "    \"colsample_bylevel\": 0.8,   # Nouveau paramètre\n",
    "    \"reg_lambda\": 10.0,         # Augmenté de 2.0 (plus de régularisation L2)\n",
    "    \"reg_alpha\": 5.0,           # Augmenté de 1.0 (plus de régularisation L1)\n",
    "    \"min_child_weight\": 3,      # Nouveau paramètre pour éviter le surapprentissage\n",
    "    \"gamma\": 0.1,               # Nouveau paramètre (minimum split loss)\n",
    "    \"random_state\": 42,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\"\n",
    "}\n",
    "\n",
    "# 6. Validation croisée temporelle avec plus de splits\n",
    "tscv = TimeSeriesSplit(n_splits=8)  # Augmenté de 5 à 8\n",
    "cv_scores = []\n",
    "val_scores = []\n",
    "\n",
    "print(\"\\n🔁 Validation croisée temporelle:\")\n",
    "for i, (train_idx, val_idx) in enumerate(tscv.split(X_train_final)):\n",
    "    X_tr, X_val = X_train_final.iloc[train_idx], X_train_final.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    model_cv = XGBRegressor(**optimized_params, early_stopping_rounds=50)\n",
    "    model_cv.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_tr, y_tr), (X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Prédictions\n",
    "    train_preds = model_cv.predict(X_tr)\n",
    "    val_preds = model_cv.predict(X_val)\n",
    "\n",
    "    # Scores\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_tr, train_preds))\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "\n",
    "    cv_scores.append(train_rmse)\n",
    "    val_scores.append(val_rmse)\n",
    "\n",
    "    print(f\"Split {i+1} - Train RMSE: {train_rmse:.5f}, Val RMSE: {val_rmse:.5f}, \"\n",
    "          f\"Écart: {val_rmse - train_rmse:.5f}\")\n",
    "\n",
    "print(f\"\\n📊 RMSE moyen Train: {np.mean(cv_scores):.5f}\")\n",
    "print(f\"📊 RMSE moyen Validation: {np.mean(val_scores):.5f}\")\n",
    "print(f\"📊 Écart moyen (surapprentissage): {np.mean(val_scores) - np.mean(cv_scores):.5f}\")\n",
    "\n",
    "# 7. Entraînement final avec early stopping plus agressif\n",
    "final_model = XGBRegressor(**optimized_params)\n",
    "final_model.fit(\n",
    "    X_train_final, y_train,\n",
    "    eval_set=[(X_train_final, y_train), (X_test_final, y_test)],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n🎯 Nombre d'arbres utilisés: {final_model}\")\n",
    "\n",
    "# 8. Prédiction & reconstruction prix J+5\n",
    "y_pred_delta = final_model.predict(X_test_final)\n",
    "close_today = df[\"Close\"].iloc[-len(y_test):].values\n",
    "y_pred_price = close_today * np.exp(y_pred_delta)\n",
    "y_true_price = close_today * np.exp(y_test.values)\n",
    "\n",
    "# 9. Évaluation détaillée\n",
    "# Sur les prix\n",
    "mae_price = mean_absolute_error(y_true_price, y_pred_price)\n",
    "rmse_price = np.sqrt(mean_squared_error(y_true_price, y_pred_price))\n",
    "r2_price = r2_score(y_true_price, y_pred_price)\n",
    "\n",
    "# Sur les log-returns\n",
    "mae_log = mean_absolute_error(y_test, y_pred_delta)\n",
    "rmse_log = np.sqrt(mean_squared_error(y_test, y_pred_delta))\n",
    "r2_log = r2_score(y_test, y_pred_delta)\n",
    "\n",
    "# Évaluation sur train pour détecter le surapprentissage\n",
    "y_pred_train_delta = final_model.predict(X_train_final)\n",
    "mae_train_log = mean_absolute_error(y_train, y_pred_train_delta)\n",
    "rmse_train_log = np.sqrt(mean_squared_error(y_train, y_pred_train_delta))\n",
    "r2_train_log = r2_score(y_train, y_pred_train_delta)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"📈 RÉSULTATS FINAUX\")\n",
    "print(\"=\"*50)\n",
    "print(f\"🏋️  TRAIN - RMSE: {rmse_train_log:.5f}, MAE: {mae_train_log:.5f}, R²: {r2_train_log:.5f}\")\n",
    "print(f\"🧪 TEST  - RMSE: {rmse_log:.5f}, MAE: {mae_log:.5f}, R²: {r2_log:.5f}\")\n",
    "print(f\"📊 ÉCART (surapprentissage): {rmse_log - rmse_train_log:.5f}\")\n",
    "\n",
    "print(f\"\\n💰 Sur les prix:\")\n",
    "print(f\"📊 MAE  : ${mae_price:.2f}\")\n",
    "print(f\"📉 RMSE : ${rmse_price:.2f}\")\n",
    "print(f\"📈 R²   : {r2_price:.4f}\")\n",
    "\n",
    "# 10. Visualisation améliorée\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Prix prédits vs réels\n",
    "axes[0,0].plot(y_true_price, label=\"Prix Réel J+5\", linewidth=2, alpha=0.8)\n",
    "axes[0,0].plot(y_pred_price, label=\"Prévision J+5\", linestyle='--', alpha=0.8)\n",
    "axes[0,0].set_title(\"📈 Prédiction du prix AAPL à J+5\")\n",
    "axes[0,0].set_xlabel(\"Observation\")\n",
    "axes[0,0].set_ylabel(\"Prix ($)\")\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot prix\n",
    "axes[0,1].scatter(y_true_price, y_pred_price, alpha=0.6, s=20)\n",
    "axes[0,1].plot([y_true_price.min(), y_true_price.max()],\n",
    "               [y_true_price.min(), y_true_price.max()], 'r--', lw=2)\n",
    "axes[0,1].set_xlabel(\"Prix Réel ($)\")\n",
    "axes[0,1].set_ylabel(\"Prix Prédit ($)\")\n",
    "axes[0,1].set_title(f\"📊 Corrélation Prix (R² = {r2_price:.3f})\")\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Log-returns\n",
    "axes[1,0].plot(y_test.values, label=\"Log-return Réel\", linewidth=2, alpha=0.8)\n",
    "axes[1,0].plot(y_pred_delta, label=\"Log-return Prédit\", linestyle='--', alpha=0.8)\n",
    "axes[1,0].set_title(\"📈 Prédiction des Log-returns\")\n",
    "axes[1,0].set_xlabel(\"Observation\")\n",
    "axes[1,0].set_ylabel(\"Log-return\")\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Résidus\n",
    "residuals = y_test.values - y_pred_delta\n",
    "axes[1,1].scatter(y_pred_delta, residuals, alpha=0.6, s=20)\n",
    "axes[1,1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1,1].set_xlabel(\"Prédictions\")\n",
    "axes[1,1].set_ylabel(\"Résidus\")\n",
    "axes[1,1].set_title(\"📊 Analyse des résidus\")\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 12. Comparaison avec baseline\n",
    "close_today_test = df[\"Close\"].iloc[-len(y_test):].values\n",
    "y_naive_price = close_today_test\n",
    "r2_naive = r2_score(y_true_price, y_naive_price)\n",
    "mae_naive = mean_absolute_error(y_true_price, y_naive_price)\n",
    "rmse_naive = np.sqrt(mean_squared_error(y_true_price, y_naive_price))\n",
    "\n",
    "print(\"\\n🧊 BASELINE NAÏVE (prix constant)\")\n",
    "print(f\"📉 RMSE : ${rmse_naive:.2f}\")\n",
    "print(f\"📊 MAE  : ${mae_naive:.2f}\")\n",
    "print(f\"📈 R²   : {r2_naive:.4f}\")\n",
    "\n",
    "# 13. Métriques de généralisation\n",
    "print(\"\\n🎯 MÉTRIQUES DE GÉNÉRALISATION\")\n",
    "print(f\"📊 Amélioration vs Baseline (RMSE): {((rmse_naive - rmse_price) / rmse_naive * 100):.1f}%\")\n",
    "print(f\"📊 Amélioration vs Baseline (MAE): {((mae_naive - mae_price) / mae_naive * 100):.1f}%\")\n",
    "\n",
    "if rmse_log - rmse_train_log < 0.001:\n",
    "    print(\"✅ Pas de surapprentissage détecté\")\n",
    "elif rmse_log - rmse_train_log < 0.005:\n",
    "    print(\"⚠️  Léger surapprentissage\")\n",
    "else:\n",
    "    print(\"❌ Surapprentissage détecté - ajuster les hyperparamètres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cfa73e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ta\n",
      "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/pc/venv/lib/python3.12/site-packages (from ta) (2.1.3)\n",
      "Requirement already satisfied: pandas in /home/pc/venv/lib/python3.12/site-packages (from ta) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/pc/venv/lib/python3.12/site-packages (from pandas->ta) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/pc/venv/lib/python3.12/site-packages (from pandas->ta) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/pc/venv/lib/python3.12/site-packages (from pandas->ta) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/pc/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n",
      "Building wheels for collected packages: ta\n",
      "  Building wheel for ta (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29482 sha256=5efb52531fcd13f4ddf1eba7f8f90beabb4ba5b774a8f64459ef88796d51a7ea\n",
      "  Stored in directory: /home/pc/.cache/pip/wheels/5c/a1/5f/c6b85a7d9452057be4ce68a8e45d77ba34234a6d46581777c6\n",
      "Successfully built ta\n",
      "Installing collected packages: ta\n",
      "Successfully installed ta-0.11.0\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mta\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmomentum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RSIIndicator\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mta\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MACD\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBRegressor, plot_importance\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TimeSeriesSplit, train_test_split\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_squared_error, r2_score\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "!pip install ta\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.trend import MACD\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler  # Plus robuste que StandardScaler\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import median_abs_deviation\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def detect_outliers_mad(data, threshold=3):\n",
    "    \"\"\"Détection d'outliers avec MAD (Median Absolute Deviation) - plus robuste que z-score\"\"\"\n",
    "    mad = median_abs_deviation(data, nan_policy='omit')\n",
    "    median = np.median(data)\n",
    "    modified_z_scores = 0.6745 * (data - median) / mad\n",
    "    return np.abs(modified_z_scores) > threshold\n",
    "\n",
    "def winsorize_series(series, lower_percentile=5, upper_percentile=95):\n",
    "    \"\"\"Winsorisation pour limiter l'impact des outliers extrêmes\"\"\"\n",
    "    lower_bound = np.percentile(series, lower_percentile)\n",
    "    upper_bound = np.percentile(series, upper_percentile)\n",
    "    return np.clip(series, lower_bound, upper_bound)\n",
    "\n",
    "def robust_feature_engineering(df):\n",
    "    \"\"\"Feature engineering robuste avec gestion des outliers\"\"\"\n",
    "\n",
    "    # Features de base avec winsorisation\n",
    "    df[\"Momentum_5\"] = df[\"Close\"] - df[\"Close\"].shift(5)\n",
    "    df[\"Momentum_5\"] = winsorize_series(df[\"Momentum_5\"])\n",
    "\n",
    "    df[\"Volatility_5\"] = df[\"Close\"].rolling(5).std()\n",
    "    df[\"Volatility_5\"] = winsorize_series(df[\"Volatility_5\"])\n",
    "\n",
    "    df[\"Return1d\"] = df[\"Close\"].pct_change()\n",
    "    df[\"Return1d\"] = winsorize_series(df[\"Return1d\"])\n",
    "\n",
    "    # Moyennes mobiles avec ratio robuste\n",
    "    df[\"MA_10\"] = df[\"Close\"].rolling(10).mean()\n",
    "    df[\"MA_50\"] = df[\"Close\"].rolling(50).mean()\n",
    "    df[\"MA_200\"] = df[\"Close\"].rolling(200).mean()\n",
    "\n",
    "    # Ratios limités pour éviter les valeurs extrêmes\n",
    "    df[\"MA_ratio\"] = np.clip(df[\"MA_10\"] / df[\"MA_50\"], 0.5, 2.0)\n",
    "\n",
    "    # Tendance long terme normalisée et bornée\n",
    "    df[\"Long_trend\"] = np.clip((df[\"Close\"] - df[\"MA_200\"]) / df[\"MA_200\"], -0.5, 0.5)\n",
    "\n",
    "    # Prix relatifs bornés\n",
    "    df[\"Price_vs_High\"] = np.clip(df[\"Close\"] / df[\"High\"].rolling(10).max(), 0.7, 1.0)\n",
    "    df[\"Price_vs_Low\"] = np.clip(df[\"Close\"] / df[\"Low\"].rolling(10).min(), 1.0, 1.5)\n",
    "\n",
    "    # Trend day normalisé et borné\n",
    "    trend_day = (df[\"Close\"] - df[\"Open\"]) / df[\"Open\"]\n",
    "    df[\"Trend_day\"] = np.clip(trend_day, -0.1, 0.1)\n",
    "\n",
    "    # RSI (déjà borné entre 0-100)\n",
    "    df[\"RSI\"] = RSIIndicator(df[\"Close\"]).rsi()\n",
    "    df[\"RSI\"] = df[\"RSI\"].fillna(50)  # Valeur neutre pour les NaN\n",
    "\n",
    "    # MACD avec winsorisation\n",
    "    macd = MACD(df[\"Close\"])\n",
    "    df[\"MACD_diff\"] = macd.macd_diff()\n",
    "    df[\"MACD_diff\"] = winsorize_series(df[\"MACD_diff\"].fillna(0))\n",
    "\n",
    "    # Volume normalisé robuste\n",
    "    volume_ma = df[\"Volume\"].rolling(20).median()  # Médiane plus robuste que moyenne\n",
    "    df[\"Volume_norm\"] = np.clip(df[\"Volume\"] / volume_ma, 0.1, 5.0)\n",
    "\n",
    "    return df\n",
    "\n",
    "# 1. Chargement et preprocessing robuste\n",
    "df = pd.read_csv(\"aapl_data.csv\").dropna()\n",
    "\n",
    "# Application du feature engineering robuste\n",
    "df = robust_feature_engineering(df)\n",
    "\n",
    "# 2. Cible avec limites pour éviter les valeurs extrêmes\n",
    "df[\"Target\"] = np.log(df[\"Close\"].shift(-5)) - np.log(df[\"Close\"])\n",
    "# Limitation des log-returns extrêmes\n",
    "df[\"Target\"] = np.clip(df[\"Target\"], -0.3, 0.3)\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"📊 Données après preprocessing: {len(df)} observations\")\n",
    "\n",
    "# 3. Sélection des features robustes\n",
    "robust_features = [\"Momentum_5\", \"Volatility_5\", \"Return1d\", \"MA_ratio\",\n",
    "                   \"Price_vs_High\", \"Price_vs_Low\", \"Trend_day\", \"Long_trend\",\n",
    "                   \"RSI\", \"MACD_diff\", \"Volume_norm\"]\n",
    "\n",
    "X = df[robust_features]\n",
    "y = df[\"Target\"]\n",
    "\n",
    "# 4. Détection et traitement des outliers avec MAD\n",
    "print(\"\\n🔍 Détection des outliers avec MAD:\")\n",
    "outlier_mask = np.zeros(len(y), dtype=bool)\n",
    "\n",
    "for col in X.columns:\n",
    "    col_outliers = detect_outliers_mad(X[col].values)\n",
    "    outlier_mask |= col_outliers\n",
    "    print(f\"  {col}: {col_outliers.sum()} outliers détectés\")\n",
    "\n",
    "target_outliers = detect_outliers_mad(y.values)\n",
    "outlier_mask |= target_outliers\n",
    "print(f\"  Target: {target_outliers.sum()} outliers détectés\")\n",
    "\n",
    "print(f\"📊 Total outliers: {outlier_mask.sum()}/{len(y)} ({outlier_mask.sum()/len(y)*100:.1f}%)\")\n",
    "\n",
    "# Conservation des données non-outliers\n",
    "X_clean = X[~outlier_mask]\n",
    "y_clean = y[~outlier_mask]\n",
    "print(f\"📊 Données finales: {len(X_clean)} observations\")\n",
    "\n",
    "# 5. Split temporel\n",
    "split_date = int(len(X_clean) * 0.8)\n",
    "X_train, X_test = X_clean.iloc[:split_date], X_clean.iloc[split_date:]\n",
    "y_train, y_test = y_clean.iloc[:split_date], y_clean.iloc[split_date:]\n",
    "\n",
    "# 6. Normalisation robuste (RobustScaler résiste mieux aux outliers)\n",
    "robust_scaler = RobustScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    robust_scaler.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    robust_scaler.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# 7. Analyse de corrélation pour la sélection finale (approche plus intelligente)\n",
    "corr_matrix = X_train_scaled.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Corrélation entre features (avec seuil plus strict)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True, linewidths=0.5)\n",
    "plt.title(\"🔍 Corrélation entre les variables explicatives\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sélection intelligente : on garde les features les plus importantes\n",
    "print(f\"\\n🔍 Matrice de corrélation:\")\n",
    "print(corr_matrix.round(2))\n",
    "\n",
    "# Stratégie : supprimer seulement si corrélation > 0.7 ET garder minimum 8 features\n",
    "corr_threshold = 0.7\n",
    "highly_correlated_pairs = []\n",
    "\n",
    "for col in upper_tri.columns:\n",
    "    correlated_features = upper_tri.index[upper_tri[col] > corr_threshold].tolist()\n",
    "    if correlated_features:\n",
    "        highly_correlated_pairs.extend([(col, feat, upper_tri.loc[feat, col]) for feat in correlated_features])\n",
    "\n",
    "# Tri par corrélation décroissante\n",
    "highly_correlated_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "print(f\"\\n🔍 Paires très corrélées (>{corr_threshold}):\")\n",
    "for pair in highly_correlated_pairs[:5]:  # Top 5\n",
    "    print(f\"  {pair[0]} - {pair[1]}: {pair[2]:.3f}\")\n",
    "\n",
    "# Suppression conservative : maximum 3 features\n",
    "to_drop = []\n",
    "if highly_correlated_pairs:\n",
    "    # On ne supprime que si on a plus de 8 features et corrélation > 0.8\n",
    "    for pair in highly_correlated_pairs:\n",
    "        if len(X_train_scaled.columns) - len(to_drop) > 8 and pair[2] > 0.8:\n",
    "            if pair[1] not in to_drop:  # Éviter les doublons\n",
    "                to_drop.append(pair[1])\n",
    "        if len(to_drop) >= 2:  # Maximum 2 suppressions\n",
    "            break\n",
    "\n",
    "print(f\"\\n🚫 Variables supprimées pour forte corrélation (>{0.8}): {to_drop}\")\n",
    "\n",
    "X_train_final = X_train_scaled.drop(columns=to_drop) if to_drop else X_train_scaled\n",
    "X_test_final = X_test_scaled.drop(columns=to_drop) if to_drop else X_test_scaled\n",
    "print(f\"🎯 Features finales ({len(X_train_final.columns)}): {list(X_train_final.columns)}\")\n",
    "\n",
    "# 8. Hyperparamètres équilibrés pour permettre l'apprentissage\n",
    "ultra_robust_params = {\n",
    "    \"n_estimators\": 300,        # Suffisant pour l'apprentissage\n",
    "    \"learning_rate\": 0.05,      # Apprentissage modéré\n",
    "    \"max_depth\": 4,             # Complexité contrôlée mais suffisante\n",
    "    \"subsample\": 0.8,           # Échantillonnage robuste\n",
    "    \"colsample_bytree\": 0.8,    # Sélection de features robuste\n",
    "    \"colsample_bylevel\": 0.8,\n",
    "    \"reg_lambda\": 5.0,          # Régularisation modérée (était trop forte)\n",
    "    \"reg_alpha\": 2.0,           # Régularisation modérée (était trop forte)\n",
    "    \"min_child_weight\": 3,      # Réduit pour permettre plus de splits\n",
    "    \"gamma\": 0.05,              # Réduit pour permettre plus de splits\n",
    "    \"max_delta_step\": 0,        # Pas de limite (0 = pas de limite)\n",
    "    \"random_state\": 42,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": [\"rmse\", \"mae\"]  # Les deux métriques pour surveillance\n",
    "}\n",
    "\n",
    "# 9. Validation croisée robuste avec bootstrapping\n",
    "def robust_cross_validation(X, y, params, n_splits=5):  # Réduit à 5 splits\n",
    "    \"\"\"Validation croisée avec mesures de robustesse\"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits, test_size=len(X)//10)  # Test size défini\n",
    "\n",
    "    mae_scores = []\n",
    "    rmse_scores = []\n",
    "    r2_scores = []\n",
    "\n",
    "    print(f\"🔁 Validation croisée avec {n_splits} splits:\")\n",
    "\n",
    "    for i, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        print(f\"  Split {i+1}: Train={len(X_tr)}, Val={len(X_val)}\")\n",
    "\n",
    "        model = XGBRegressor(**params, early_stopping_rounds=30)\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "        preds = model.predict(X_val)\n",
    "\n",
    "        mae = mean_absolute_error(y_val, preds)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "        r2 = r2_score(y_val, preds)\n",
    "\n",
    "        mae_scores.append(mae)\n",
    "        rmse_scores.append(rmse)\n",
    "        r2_scores.append(r2)\n",
    "\n",
    "        print(f\"    MAE: {mae:.5f}, RMSE: {rmse:.5f}, R²: {r2:.5f}\")\n",
    "\n",
    "    return {\n",
    "        'mae_mean': np.mean(mae_scores),\n",
    "        'mae_std': np.std(mae_scores),\n",
    "        'rmse_mean': np.mean(rmse_scores),\n",
    "        'rmse_std': np.std(rmse_scores),\n",
    "        'r2_mean': np.mean(r2_scores),\n",
    "        'r2_std': np.std(r2_scores)\n",
    "    }\n",
    "\n",
    "print(\"\\n🔁 Validation croisée robuste en cours...\")\n",
    "cv_results = robust_cross_validation(X_train_final, y_train, ultra_robust_params)\n",
    "\n",
    "print(f\"\\n📊 RÉSULTATS VALIDATION CROISÉE:\")\n",
    "print(f\"MAE  : {cv_results['mae_mean']:.5f} ± {cv_results['mae_std']:.5f}\")\n",
    "print(f\"RMSE : {cv_results['rmse_mean']:.5f} ± {cv_results['rmse_std']:.5f}\")\n",
    "print(f\"R²   : {cv_results['r2_mean']:.5f} ± {cv_results['r2_std']:.5f}\")\n",
    "\n",
    "# 10. Entraînement du modèle final avec surveillance\n",
    "print(f\"\\n🎯 Entraînement du modèle final avec {len(X_train_final.columns)} features\")\n",
    "print(f\"📊 Taille train: {len(X_train_final)}, Taille test: {len(X_test_final)}\")\n",
    "\n",
    "# Vérification des données avant entraînement\n",
    "print(f\"📊 Plage cible train: [{y_train.min():.4f}, {y_train.max():.4f}]\")\n",
    "print(f\"📊 Plage cible test: [{y_test.min():.4f}, {y_test.max():.4f}]\")\n",
    "\n",
    "final_model = XGBRegressor(**ultra_robust_params, early_stopping_rounds=50)  # Plus tolérant\n",
    "final_model.fit(\n",
    "    X_train_final, y_train,\n",
    "    eval_set=[(X_train_final, y_train), (X_test_final, y_test)],\n",
    "    verbose=10  # Affichage toutes les 10 itérations pour diagnostic\n",
    ")\n",
    "\n",
    "print(f\"\\n🎯 Modèle entraîné avec {final_model.best_iteration} arbres\")\n",
    "print(f\"🎯 Meilleur score: {final_model.best_score:.5f}\")\n",
    "\n",
    "# 11. Évaluation finale avec métriques robustes\n",
    "y_pred_train = final_model.predict(X_train_final)\n",
    "y_pred_test = final_model.predict(X_test_final)\n",
    "\n",
    "# Métriques sur log-returns\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "# 12. Test de robustesse sur prix\n",
    "close_train = df[\"Close\"].iloc[:len(y_train)].values\n",
    "close_test = df[\"Close\"].iloc[-len(y_test):].values\n",
    "\n",
    "price_pred_train = close_train * np.exp(y_pred_train)\n",
    "price_true_train = close_train * np.exp(y_train.values)\n",
    "price_pred_test = close_test * np.exp(y_pred_test)\n",
    "price_true_test = close_test * np.exp(y_test.values)\n",
    "\n",
    "price_mae_train = mean_absolute_error(price_true_train, price_pred_train)\n",
    "price_mae_test = mean_absolute_error(price_true_test, price_pred_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🛡️  ÉVALUATION DE ROBUSTESSE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"📊 TRAIN - MAE: {train_mae:.5f}, RMSE: {train_rmse:.5f}, R²: {train_r2:.5f}\")\n",
    "print(f\"🧪 TEST  - MAE: {test_mae:.5f}, RMSE: {test_rmse:.5f}, R²: {test_r2:.5f}\")\n",
    "print(f\"📈 ÉCART MAE (robustesse): {abs(test_mae - train_mae):.5f}\")\n",
    "print(f\"📈 ÉCART RMSE (robustesse): {abs(test_rmse - train_rmse):.5f}\")\n",
    "\n",
    "print(f\"\\n💰 PRIX - Train MAE: ${price_mae_train:.2f}, Test MAE: ${price_mae_test:.2f}\")\n",
    "print(f\"💰 ÉCART PRIX: ${abs(price_mae_test - price_mae_train):.2f}\")\n",
    "\n",
    "# 13. Indicateurs de robustesse\n",
    "robustness_score = 1 - abs(test_mae - train_mae) / train_mae\n",
    "stability_score = 1 - cv_results['mae_std'] / cv_results['mae_mean']\n",
    "\n",
    "print(f\"\\n🎯 SCORES DE ROBUSTESSE:\")\n",
    "print(f\"Robustesse Train/Test: {robustness_score:.3f} (1.0 = parfait)\")\n",
    "print(f\"Stabilité CV: {stability_score:.3f} (1.0 = parfait)\")\n",
    "\n",
    "if robustness_score > 0.95 and stability_score > 0.9:\n",
    "    print(\"✅ MODÈLE TRÈS ROBUSTE\")\n",
    "elif robustness_score > 0.9 and stability_score > 0.8:\n",
    "    print(\"✅ MODÈLE ROBUSTE\")\n",
    "elif robustness_score > 0.8 and stability_score > 0.7:\n",
    "    print(\"⚠️  MODÈLE MOYENNEMENT ROBUSTE\")\n",
    "else:\n",
    "    print(\"❌ MODÈLE PEU ROBUSTE - Révision nécessaire\")\n",
    "\n",
    "# 14. Visualisations de robustesse\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Performance train vs test\n",
    "axes[0,0].scatter(y_train, y_pred_train, alpha=0.5, s=20, label=f'Train (MAE={train_mae:.4f})')\n",
    "axes[0,0].scatter(y_test, y_pred_test, alpha=0.5, s=20, label=f'Test (MAE={test_mae:.4f})')\n",
    "axes[0,0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "axes[0,0].set_xlabel(\"Valeurs Réelles\")\n",
    "axes[0,0].set_ylabel(\"Prédictions\")\n",
    "axes[0,0].set_title(\"🎯 Robustesse Train vs Test\")\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Résidus pour détecter les patterns\n",
    "residuals_test = y_test.values - y_pred_test\n",
    "axes[0,1].scatter(y_pred_test, residuals_test, alpha=0.6, s=20)\n",
    "axes[0,1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0,1].set_xlabel(\"Prédictions\")\n",
    "axes[0,1].set_ylabel(\"Résidus\")\n",
    "axes[0,1].set_title(\"📊 Analyse des Résidus\")\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution des erreurs\n",
    "axes[0,2].hist(residuals_test, bins=30, alpha=0.7, density=True)\n",
    "axes[0,2].axvline(x=0, color='r', linestyle='--')\n",
    "axes[0,2].set_xlabel(\"Résidus\")\n",
    "axes[0,2].set_ylabel(\"Densité\")\n",
    "axes[0,2].set_title(\"📈 Distribution des Erreurs\")\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# Prix prédits vs réels\n",
    "axes[1,0].plot(price_true_test, label=\"Prix Réel\", linewidth=2, alpha=0.8)\n",
    "axes[1,0].plot(price_pred_test, label=\"Prix Prédit\", linestyle='--', alpha=0.8)\n",
    "axes[1,0].set_title(\"💰 Prédiction des Prix\")\n",
    "axes[1,0].set_xlabel(\"Observation\")\n",
    "axes[1,0].set_ylabel(\"Prix ($)\")\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Importance des features\n",
    "feature_importance = final_model.feature_importances_\n",
    "feature_names = X_train_final.columns\n",
    "sorted_idx = np.argsort(feature_importance)[-10:]\n",
    "axes[1,1].barh(range(len(sorted_idx)), feature_importance[sorted_idx])\n",
    "axes[1,1].set_yticks(range(len(sorted_idx)))\n",
    "axes[1,1].set_yticklabels([feature_names[i] for i in sorted_idx])\n",
    "axes[1,1].set_title(\"🎯 Importance des Features\")\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Performance temporelle\n",
    "error_abs = np.abs(residuals_test)\n",
    "axes[1,2].plot(error_abs, alpha=0.7)\n",
    "axes[1,2].axhline(y=np.mean(error_abs), color='r', linestyle='--',\n",
    "                  label=f'MAE moyen: {np.mean(error_abs):.4f}')\n",
    "axes[1,2].set_xlabel(\"Observation\")\n",
    "axes[1,2].set_ylabel(\"Erreur Absolue\")\n",
    "axes[1,2].set_title(\"📉 Stabilité Temporelle\")\n",
    "axes[1,2].legend()\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 15. Test de stress (simulation avec bruit)\n",
    "print(\"\\n🧪 TEST DE STRESS - Ajout de bruit gaussien\")\n",
    "noise_levels = [0.01, 0.02, 0.05, 0.1]\n",
    "stress_results = []\n",
    "\n",
    "for noise in noise_levels:\n",
    "    X_test_noisy = X_test_final + np.random.normal(0, noise, X_test_final.shape)\n",
    "    y_pred_noisy = final_model.predict(X_test_noisy)\n",
    "    mae_noisy = mean_absolute_error(y_test, y_pred_noisy)\n",
    "    stress_results.append(mae_noisy)\n",
    "    print(f\"Bruit {noise:.2f}: MAE = {mae_noisy:.5f} (dégradation: {((mae_noisy-test_mae)/test_mae*100):+.1f}%)\")\n",
    "\n",
    "print(f\"\\n🛡️  RÉSISTANCE AU BRUIT: Dégradation moyenne {np.mean([(s-test_mae)/test_mae*100 for s in stress_results]):.1f}%\")\n",
    "import os\n",
    "\n",
    "# Créer le dossier 'model' s'il n'existe pas\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Enregistrement du modèle\n",
    "joblib.dump(final_model, \"model/final_model.xgb\")\n",
    "\n",
    "# Enregistrement du scaler (pour les nouvelles données)\n",
    "joblib.dump(robust_scaler, \"model/scaler.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
